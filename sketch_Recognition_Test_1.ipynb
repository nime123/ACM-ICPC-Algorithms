{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sketch Recognition Test 1",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPtEmRIiZaXIV+fhvAZwQON",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimesha-nishadhi/ACM-ICPC-Algorithms/blob/master/sketch_Recognition_Test_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB7DRvc_NMia"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qi45FWfbW7HJ",
        "outputId": "77aec9a3-453b-468b-9a44-7591bdf7a2bf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras \n",
        "import sklearn\n",
        "from sklearn.model_selection import KFold\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "#import urllib\n",
        "import urllib.request\n",
        "\n",
        "classes = ['cat','ant','banana','bread']\n",
        "\n",
        "url = 'https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/'\n",
        "# Download the data of the aforementioned classes\n",
        "for clas in classes:\n",
        "\tcomplete_url = url+clas+\".npy\"\n",
        "\tprint(\"Downloading = \",complete_url)\n",
        "\turllib.request.urlretrieve(complete_url, \"./\"+clas+\".npy\")\n",
        "\n",
        "# Grep all the downloaded files and add them to a list\n",
        "data_sets = glob.glob(os.path.join('./*.npy'))\n",
        "\n",
        "#initialize variables \n",
        "input = np.empty([0, 784]) # Train data\n",
        "labels = np.empty([0])\t# Test data\n",
        "\n",
        "index = 0\n",
        "# Concat the train and test data from all the files\n",
        "for file in data_sets:\n",
        "\tdata = np.load(file)\n",
        "\tdata = data[0: 6000, :]\n",
        "\tinput = np.concatenate((input, data), axis=0)\n",
        "\tlabels = np.append(labels, [index]*data.shape[0])\n",
        "\tindex += 1\n",
        "\n",
        "'''\n",
        "\tK-Folds cross-validator\n",
        "\tn_splits : Number of folds to be used\n",
        "'''\n",
        "n_fold = 5\n",
        "kf = KFold(n_splits=n_fold,shuffle=True,random_state=9)\n",
        "x_train = None\n",
        "x_test = None\n",
        "y_train = None\n",
        "y_test = None\n",
        "random_ordering = np.random.permutation(input.shape[0])\n",
        "input = input[random_ordering, :]\n",
        "labels = labels[random_ordering]\n",
        "for train_index, test_index in kf.split(input):\n",
        "    # Divide the dataset into train and test\n",
        "    x_train, x_test = input[train_index], input[test_index]\n",
        "    y_train, y_test = labels[train_index], labels[test_index]\n",
        "    break\n",
        "\n",
        "# Reshape the image size to be 28 x 28 \n",
        "image_size = 28\n",
        "x_train = x_train.reshape(x_train.shape[0], image_size, image_size, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], image_size, image_size, 1)\n",
        "\n",
        "# Divide all the values by 255 to normalize the image\n",
        "x_train /= 255.00\n",
        "x_test /= 255.00\n",
        "num_classes = len(classes)\n",
        "\n",
        "\n",
        "# CNN Model\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Convolution2D(64, (3, 3),\n",
        "                        padding='same',\n",
        "                        input_shape=x_train.shape[1:], activation='relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(3, 3)))\n",
        "model.add(layers.Convolution2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(3, 3)))\n",
        "model.add(layers.Convolution2D(64, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size =(3,3)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(num_classes, activation='softmax')) \n",
        "optimizer = tf.optimizers.Adam()\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])\n",
        "# Fit a model to the train data\n",
        "model.fit(x = x_train, y = y_train, batch_size = 100,  validation_split = 0.2, epochs=15)\n",
        "\n",
        "# Obtain the accuracy of the above model on the test data\n",
        "accuracy = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy',accuracy[1] * 100)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading =  https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy\n",
            "Downloading =  https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/ant.npy\n",
            "Downloading =  https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/banana.npy\n",
            "Downloading =  https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/bread.npy\n",
            "Epoch 1/15\n",
            "154/154 [==============================] - 23s 142ms/step - loss: 0.5348 - accuracy: 0.8015 - val_loss: 0.3199 - val_accuracy: 0.8878\n",
            "Epoch 2/15\n",
            "154/154 [==============================] - 22s 141ms/step - loss: 0.2715 - accuracy: 0.9064 - val_loss: 0.2350 - val_accuracy: 0.9172\n",
            "Epoch 3/15\n",
            "154/154 [==============================] - 22s 141ms/step - loss: 0.2028 - accuracy: 0.9309 - val_loss: 0.2121 - val_accuracy: 0.9245\n",
            "Epoch 4/15\n",
            "154/154 [==============================] - 22s 140ms/step - loss: 0.1671 - accuracy: 0.9442 - val_loss: 0.1931 - val_accuracy: 0.9320\n",
            "Epoch 5/15\n",
            "154/154 [==============================] - 22s 141ms/step - loss: 0.1381 - accuracy: 0.9536 - val_loss: 0.1918 - val_accuracy: 0.9331\n",
            "Epoch 6/15\n",
            "154/154 [==============================] - 22s 141ms/step - loss: 0.1255 - accuracy: 0.9579 - val_loss: 0.1722 - val_accuracy: 0.9427\n",
            "Epoch 7/15\n",
            "154/154 [==============================] - 22s 140ms/step - loss: 0.0995 - accuracy: 0.9671 - val_loss: 0.1771 - val_accuracy: 0.9417\n",
            "Epoch 8/15\n",
            "154/154 [==============================] - 22s 140ms/step - loss: 0.0920 - accuracy: 0.9688 - val_loss: 0.1685 - val_accuracy: 0.9432\n",
            "Epoch 9/15\n",
            "154/154 [==============================] - 22s 140ms/step - loss: 0.0723 - accuracy: 0.9742 - val_loss: 0.1812 - val_accuracy: 0.9398\n",
            "Epoch 10/15\n",
            "154/154 [==============================] - 21s 140ms/step - loss: 0.0609 - accuracy: 0.9789 - val_loss: 0.1849 - val_accuracy: 0.9398\n",
            "Epoch 11/15\n",
            "154/154 [==============================] - 21s 140ms/step - loss: 0.0535 - accuracy: 0.9826 - val_loss: 0.1993 - val_accuracy: 0.9419\n",
            "Epoch 12/15\n",
            "154/154 [==============================] - 21s 140ms/step - loss: 0.0379 - accuracy: 0.9882 - val_loss: 0.1795 - val_accuracy: 0.9492\n",
            "Epoch 13/15\n",
            "154/154 [==============================] - 21s 139ms/step - loss: 0.0261 - accuracy: 0.9914 - val_loss: 0.2127 - val_accuracy: 0.9453\n",
            "Epoch 14/15\n",
            "154/154 [==============================] - 21s 139ms/step - loss: 0.0256 - accuracy: 0.9915 - val_loss: 0.2946 - val_accuracy: 0.9219\n",
            "Epoch 15/15\n",
            "154/154 [==============================] - 22s 140ms/step - loss: 0.0278 - accuracy: 0.9900 - val_loss: 0.2449 - val_accuracy: 0.9424\n",
            "150/150 [==============================] - 2s 14ms/step - loss: 0.2129 - accuracy: 0.9485\n",
            "Test accuracy 94.85416412353516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5QKMRDQ-v-_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIw8fLaO_3Jf"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "BrD5svmd-w4_",
        "outputId": "8a27b798-60d2-435f-b5f7-27dc3612cc54"
      },
      "source": [
        "\"\"\"Sketch-RNN Model.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import random\n",
        "\n",
        "from magenta.models.sketch_rnn import rnn\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def copy_hparams(hparams):\n",
        "  \"\"\"Return a copy of an HParams instance.\"\"\"\n",
        "  return tf.contrib.training.HParams(**hparams.values())\n",
        "\n",
        "\n",
        "def get_default_hparams():\n",
        "  \"\"\"Return default HParams for sketch-rnn.\"\"\"\n",
        "  hparams = tf.contrib.training.HParams(\n",
        "      data_set=['sketchrnn_cat.npz','sketchrnn_dog.npz','sketchrnn_bear.npz','sketchrnn_airplane.npz',\n",
        "                'sketchrnn_ant.npz','sketchrnn_banana.npz','sketchrnn_bench.npz','sketchrnn_book.npz',\n",
        "                'sketchrnn_bottlecap.npz','sketchrnn_bread.npz'],  # Our dataset.\n",
        "      # data_set=['aaron_sheep/aaron_sheep.npz','kanji/short_kanji.npz','omniglot/omniglot.npz'],\n",
        "      num_steps=10000,  # Total number of steps of training. Keep large.\n",
        "      save_every=50,  # Number of batches per checkpoint creation.\n",
        "      max_seq_len=250,  # Not used. Will be changed by model. [Eliminate?]\n",
        "      dec_rnn_size=512,  # Size of decoder.\n",
        "      dec_model='lstm',  # Decoder: lstm, layer_norm or hyper.\n",
        "      enc_rnn_size=256,  # Size of encoder.\n",
        "      enc_model='lstm',  # Encoder: lstm, layer_norm or hyper.\n",
        "      z_size=128,  # Size of latent vector z. Recommend 32, 64 or 128.\n",
        "      kl_weight=0.5,  # KL weight of loss equation. Recommend 0.5 or 1.0.\n",
        "      kl_weight_start=0.01,  # KL start weight when annealing.\n",
        "      kl_tolerance=0.2,  # Level of KL loss at which to stop optimizing for KL.\n",
        "      batch_size=100,  # Minibatch size. Recommend leaving at 100.\n",
        "      grad_clip=1.0,  # Gradient clipping. Recommend leaving at 1.0.\n",
        "      num_mixture=20,  # Number of mixtures in Gaussian mixture model.\n",
        "      learning_rate=0.001,  # Learning rate.\n",
        "      decay_rate=0.9999,  # Learning rate decay per minibatch.\n",
        "      kl_decay_rate=0.99995,  # KL annealing decay rate per minibatch.\n",
        "      min_learning_rate=0.00001,  # Minimum learning rate.\n",
        "      use_recurrent_dropout=True,  # Dropout with memory loss. Recomended\n",
        "      recurrent_dropout_prob=0.90,  # Probability of recurrent dropout keep.\n",
        "      use_input_dropout=False,  # Input dropout. Recommend leaving False.\n",
        "      input_dropout_prob=0.90,  # Probability of input dropout keep.\n",
        "      use_output_dropout=False,  # Output droput. Recommend leaving False.\n",
        "      output_dropout_prob=0.90,  # Probability of output dropout keep.\n",
        "      random_scale_factor=0.15,  # Random scaling data augmention proportion.\n",
        "      augment_stroke_prob=0.10,  # Point dropping augmentation proportion.\n",
        "      conditional=True,  # When False, use unconditional decoder-only model.\n",
        "      is_training=True,  # Is model training? Recommend keeping true.\n",
        "      loss_function='softmax', # Loss function being used for classification.\n",
        "      num_classes = 10 # Number of classes predictions.\n",
        "  )\n",
        "  return hparams\n",
        "\n",
        "\n",
        "class Model(object):\n",
        "  \"\"\"Define a SketchRNN model.\"\"\"\n",
        "\n",
        "  def __init__(self, hps, gpu_mode=True, reuse=False):\n",
        "    \"\"\"Initializer for the SketchRNN model.\n",
        "    Args:\n",
        "       hps: a HParams object containing model hyperparameters\n",
        "       gpu_mode: a boolean that when True, uses GPU mode.\n",
        "       reuse: a boolean that when true, attemps to reuse variables.\n",
        "    \"\"\"\n",
        "    self.hps = hps\n",
        "    with tf.variable_scope('vector_rnn', reuse=reuse):\n",
        "      if not gpu_mode:\n",
        "        with tf.device('/cpu:0'):\n",
        "          tf.logging.info('Model using cpu.')\n",
        "          self.build_model(hps)\n",
        "      else:\n",
        "        tf.logging.info('Model using gpu.')\n",
        "        self.build_model(hps)\n",
        "\n",
        "  def encoder(self, batch, sequence_lengths):\n",
        "    \"\"\"Define the bi-directional encoder module of sketch-rnn.\"\"\"\n",
        "    unused_outputs, last_states = tf.nn.bidirectional_dynamic_rnn(\n",
        "        self.enc_cell_fw,\n",
        "        self.enc_cell_bw,\n",
        "        batch,\n",
        "        sequence_length=sequence_lengths,\n",
        "        time_major=False,\n",
        "        swap_memory=True,\n",
        "        dtype=tf.float32,\n",
        "        scope='ENC_RNN')\n",
        "    last_state_fw, last_state_bw = last_states\n",
        "    last_h_fw = self.enc_cell_fw.get_output(last_state_fw)\n",
        "    last_h_bw = self.enc_cell_bw.get_output(last_state_bw)\n",
        "    last_h = tf.concat([last_h_fw, last_h_bw], 1)\n",
        "    \n",
        "    # Removed the decoder part from the actual sketchrnn code \n",
        "    # and just returning last_h\n",
        "    return last_h\n",
        "\n",
        "  def build_model(self, hps):\n",
        "    \"\"\"Define model architecture.\"\"\"\n",
        "    if hps.is_training:\n",
        "      self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "\n",
        "    if hps.enc_model == 'lstm':\n",
        "      enc_cell_fn = rnn.LSTMCell\n",
        "    elif hps.enc_model == 'layer_norm':\n",
        "      enc_cell_fn = rnn.LayerNormLSTMCell\n",
        "    elif hps.enc_model == 'hyper':\n",
        "      enc_cell_fn = rnn.HyperLSTMCell\n",
        "    else:\n",
        "      assert False, 'please choose a respectable cell'\n",
        "\n",
        "    use_recurrent_dropout = self.hps.use_recurrent_dropout\n",
        "    use_input_dropout = self.hps.use_input_dropout\n",
        "    use_output_dropout = self.hps.use_output_dropout\n",
        "\n",
        "    if hps.conditional:  # vae mode:\n",
        "      if hps.enc_model == 'hyper':\n",
        "        self.enc_cell_fw = enc_cell_fn(\n",
        "            hps.enc_rnn_size,\n",
        "            use_recurrent_dropout=use_recurrent_dropout,\n",
        "            dropout_keep_prob=self.hps.recurrent_dropout_prob)\n",
        "        self.enc_cell_bw = enc_cell_fn(\n",
        "            hps.enc_rnn_size,\n",
        "            use_recurrent_dropout=use_recurrent_dropout,\n",
        "            dropout_keep_prob=self.hps.recurrent_dropout_prob)\n",
        "      else:\n",
        "        self.enc_cell_fw = enc_cell_fn(\n",
        "            hps.enc_rnn_size,\n",
        "            use_recurrent_dropout=use_recurrent_dropout,\n",
        "            dropout_keep_prob=self.hps.recurrent_dropout_prob)\n",
        "        self.enc_cell_bw = enc_cell_fn(\n",
        "            hps.enc_rnn_size,\n",
        "            use_recurrent_dropout=use_recurrent_dropout,\n",
        "            dropout_keep_prob=self.hps.recurrent_dropout_prob)\n",
        "\n",
        "    self.sequence_lengths = tf.placeholder(dtype=tf.int32, shape=[self.hps.batch_size])\n",
        "    self.input_data = tf.placeholder(dtype=tf.float32, shape=[self.hps.batch_size, self.hps.max_seq_len + 1, 5])\n",
        "    self.y_labels = tf.placeholder(dtype=tf.int32, shape=[self.hps.batch_size])\n",
        "    print(\"self.y_labels.shape = \",self.y_labels.shape)\n",
        "    # The target/expected vectors of strokes\n",
        "    self.output_x = self.input_data[:, 1:self.hps.max_seq_len + 1, :]\n",
        "    \n",
        "    # either do vae-bit and get z, or do unconditional, decoder-only\n",
        "    if hps.conditional:  # vae mode:\n",
        "      self.batch_z = self.encoder(self.output_x, self.sequence_lengths)\n",
        "    else:  # unconditional, decoder-only generation\n",
        "      self.batch_z = tf.zeros((self.hps.batch_size, self.hps.z_size), dtype=tf.float32)\n",
        "\n",
        "\n",
        "    # TODO(deck): Better understand this comment.\n",
        "    # Number of outputs is 3 (one logit per pen state) plus 6 per mixture\n",
        "    # component: mean_x, stdev_x, mean_y, stdev_y, correlation_xy, and the\n",
        "    # mixture weight/probability (Pi_k)\n",
        "    n_out = self.hps.num_classes #num_classes\n",
        "\n",
        "    with tf.variable_scope('RNN'):\n",
        "      output_w = tf.get_variable('output_w', [2*self.hps.enc_rnn_size, n_out])\n",
        "      output_b = tf.get_variable('output_b', [n_out])\n",
        "\n",
        "    output = tf.nn.xw_plus_b(self.batch_z, output_w, output_b)\n",
        "    self.output = output\n",
        "    if self.y_labels is not None:\n",
        "      self.ce_loss = self.lossfunctions(self.hps.loss_function)\n",
        "    else:\n",
        "      self.ce_loss = 0\n",
        "    if self.hps.is_training:\n",
        "      self.lr = tf.Variable(self.hps.learning_rate, trainable=False)\n",
        "      optimizer = tf.train.AdamOptimizer(self.lr)\n",
        "\n",
        "      self.cost = self.ce_loss\n",
        "\n",
        "      gvs = optimizer.compute_gradients(self.cost)\n",
        "      g = self.hps.grad_clip\n",
        "      capped_gvs = [(tf.clip_by_value(grad, -g, g), var) for grad, var in gvs]\n",
        "      self.train_op = optimizer.apply_gradients(capped_gvs, global_step=self.global_step, name='train_step')\n",
        "\n",
        "  def lossfunctions(self, lossfn):\n",
        "    if lossfn == 'softmax':\n",
        "      return tf.reduce_mean(\n",
        "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          logits=self.output,\n",
        "          labels=self.y_labels\n",
        "          )\n",
        "        )\n",
        "    # elif lossfn == 'sigmoid':\n",
        "    #   loss_val = tf.constant(0.0)\n",
        "    #   for i in range(self.hps.num_classes):\n",
        "    #     loss_val = tf.add(loss_val,tf.reduce_mean(\n",
        "    #     tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "    #       _sentinel=None,\n",
        "    #       labels=tf.cast(tf.reshape(self.y_labels,[self.y_labels.shape[0],1]),tf.float32),\n",
        "    #       logits=tf.transpose(tf.gather_nd(\n",
        "    #         tf.transpose(self.output),\n",
        "    #         [[i]],\n",
        "    #         name=None\n",
        "    #         )\n",
        "    #       ),\n",
        "    #       name=None\n",
        "    #       )\n",
        "    #     )\n",
        "    #    )\n",
        "    #   return loss_val/self.hps.num_classes\n",
        "    # elif lossfn == 'weighted':\n",
        "    #   loss_val = tf.constant(0.0)\n",
        "    #   for i in range(self.hps.num_classes):\n",
        "    #     loss_val = tf.add(loss_val,tf.reduce_mean(\n",
        "    #     tf.nn.weighted_cross_entropy_with_logits(\n",
        "    #       targets=tf.cast(tf.reshape(self.y_labels,[self.y_labels.shape[0],1]),tf.float32),\n",
        "    #       logits=tf.transpose(tf.gather_nd(\n",
        "    #         tf.transpose(self.output),\n",
        "    #         [[i]],\n",
        "    #         name=None\n",
        "    #         )\n",
        "    #       ),\n",
        "    #       pos_weight=tf.constant(0.9)\n",
        "    #       )\n",
        "    #     )\n",
        "    #     )\n",
        "    #   return loss_val\n",
        "    # else:\n",
        "    #   assert False, 'Please choose from the following lossfunctions:\\n \\\n",
        "    #   1. softmax \\n 2. sigmoid \\n 3. weighted \\n'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f85b24907bbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmagenta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msketch_rnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'magenta'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}